{{- /* Routing templates: InferencePool, InferenceModel, and HttpRoute */}}
{{- if .Values.inferencePool }}
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferencePool
metadata:
  name: {{ include "llm-d-modelservice.fullname" . }}-inference-pool
  namespace: {{ .Release.Namespace }}
spec:
  extensionRef:
    failureMode: FailClose
    group: ""
    kind: Service
    name: {{ include "llm-d-modelservice.fullname" . }}-epp-service
  selector:
    {{- if .Values.multinode }}
    leaderworkerset.sigs.k8s.io/worker-index: "0"
    {{- end }}
    {{- include "llm-d-modelservice.pdlabels" . | nindent 4 }}
  targetPortNumber: 8000
{{- end }}
--- 
{{- if .Values.inferenceModel }}
apiVersion: inference.networking.x-k8s.io/v1alpha2
kind: InferenceModel
metadata:
  name: {{ include "llm-d-modelservice.fullname" . }}-inference-model
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "llm-d-modelservice.pdlabels" . | nindent 4 }}
spec:
  modelName: {{ .Values.routing.modelName }}
  poolRef:
    group: inference.networking.x-k8s.io
    kind: InferencePool
    name: {{ include "llm-d-modelservice.fullname" . }}-inference-pool
{{- end }}
--- 
{{- if .Values.httpRoute }}
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
  name: {{ include "llm-d-modelservice.fullname" . }}-http-route
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "llm-d-modelservice.labels" . | nindent 4 }}
spec:
  {{- with .Values.routing.parentRefs }}
  parentRefs:
  {{- . | toYaml | nindent 2}}
  {{- end }}
  rules:
  - backendRefs:
    - group: inference.networking.x-k8s.io
      kind: InferencePool
      name: {{ include "llm-d-modelservice.fullname" . }}-inference-pool
      port: {{ .Values.routing.servicePort  }}
      weight: 1
    matches:
    - headers:
      - name: x-model-name
        type: Exact
        value: {{ .Values.routing.modelName }}
      path:
        type: PathPrefix
        value: /
{{- end }}