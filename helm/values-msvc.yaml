# TODO
# decoupleScaling: false

modelServiceName: facebook-opt-125m       # DNS compliant
lws: false  # If true, creates LWS instead of deployments  
inferencePool: true 
inferenceModel: true 
httpRoute: true 
routing: 
  # This is the model name for the OpenAI request
  modelName: facebook/opt-125m
  ports:
    servicePort: 8000   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
    internalPort: 8200  # Sidecar forwards request to vllm container on this port 

modelArtifacts:
  # When specfying the URI with `hf` prefix, the <repo-id>/<model-id> string
  # is extracted and exposed as a template variable that can be used as {{ .HFModelName }}
  
  # uri: hf://facebook/opt-125m
  type: hf                # oneOf ["hf", "oci", "pvc"]
  artficat: facebook/opt-125m
  authSecretName: "hf-secret"
  size: 5Mi 
  gatewayRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway-kgateway

# describe decode pods
decode:
  enableService: false
  replicas: 1
  
  # for LWS
  parallelism:  
    tensor: 8
    data: 16
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: nvidia.com/gpu.product
    labelValues:
      # According to the blog, Scout requires H100s
      - NVIDIA-H100
  # initContainers:
  containers:
  - name: "vllm"
    image: "vllm-ai/vllm:latest"  
    args:
      - "{{ .HFModelName }}"
    env:
    - name: "VLLM_LOG_LEVEL"
      value: "DEBUG"  # Set to DEBUG for more detailed logs, or INFO for less verbose logs
    envFrom:
      - configMapRef:
          name: vllm-config
    resources:
      requests:
        cpu: "1"          # Request 1 CPU core
        memory: "4Gi"    # Request 4 GiB of memory
      limits:
        cpu: "2"          # Limit to 2 CPU cores
        memory: "8Gi"     # Limit to 8 GiB of memory
    mountModelVolume: true

# describe the prefill pods (looks the same as above)
prefill:
  replicas: 1
  containers:
    - name: "vllm"
      args:
        - "{{ .HFModelName }}"
    
endpointPicker:
  # Same fields as decode and prefill sections
  containers:
  - name: "epp"
    env:
    - name: HF_TOKEN
      value: hello
    - name: USE_STREAMING
      value: "false"

