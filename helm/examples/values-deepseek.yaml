# This values.yaml file creates the resources for deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct

# If true, use a LeaderWorkerSet instead of a Deployment to host the model
multinode: true
inferencePool: true
inferenceModel: true
httpRoute: true

routing: 
  # This is the model name for the OpenAI request
  modelName: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
  servicePort: 8080   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
    debugLevel: 5
  parentRefs:
  - group: gateway.networking.k8s.io
    kind: Gateway
    name: inference-gateway

modelArtifacts:
  # When specfying the URI with `hf` prefix, the <repo-id>/<model-id> string
  # is extracted and exposed as a template variable that can be used as {{ .HFModelName }}
  
  uri: "hf://deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct"
  authSecretName: "hf-secret"
  size: 5Mi 

# describe decode pods
decode:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    tensor: 2
    data: 2
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-base:0.0.15"
      imagePullPolicy: Always
      workingDir: /app
      stdin: true
      tty: true
      command: ["/bin/sh","-c"]
      args:
        - |
          # Squash a warning.
          rm /etc/libibverbs.d/vmw_pvrdma.driver
          #################
          # Install vLLM
          #################
          /init-scripts/init-vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              #### MK
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              #### MK
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --kv-transfer-config \
                '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
              --enforce-eager
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              #### MK
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              #### MK
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --kv-transfer-config \
                '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
              --enforce-eager \
              --headless
          fi
      env:
        - name: DP_SIZE
          value: "{{ .DecodeDataParallelism }}"
        - name: TP_SIZE
          value: "{{ .DecodeTensorParallelism }}"
        - name: DP_SIZE_LOCAL
          value: "1"
        - name: VLLM_REPO_URL
          value: "https://github.com/vllm-project/vllm.git"
        - name: VLLM_BRANCH
          value: "main"
        - name: VLLM_ALL2ALL_BACKEND
#                    value: "naive"
          value: "pplx"
#                    value: "deepep_high_throughput"
#                    value: "deepep_low_latency"
#
          # Needed for GDRCOPY to be used.
          # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
#                  - name: NVIDIA_NVSWITCH
#                    value: "enabled"
#                  - name: NVIDIA_GDS
#                    value: "enabled"

        # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
#                  - name: NVIDIA_MOFED
#                    value: "enabled"
#
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NVSHMEM_DEBUG
          value: "TRACE"
        - name: NVSHMEM_DEBUG_SUBSYS
          value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibrc"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
          value: "true"
        - name: NVSHMEM_HCA_LIST
          value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-secret
              key: HF_TOKEN
              optional: true
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add: [ "IPC_LOCK" ]
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 64Gi
          ephemeral-storage: 256Gi
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 64Gi
          ephemeral-storage: 256Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
      mountModelVolume: true

# describe prefill pods
prefill:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    tensor: 2
    data: 2
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  volumes:
    - configMap:
        defaultMode: 493
        name: vllm-init-scripts-config
      name: init-scripts-volume
    - emptyDir:
        medium: Memory
        sizeLimit: 1Gi
      name: dshm


  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-base:0.0.15"
      imagePullPolicy: Always
      workingDir: /app
      stdin: true
      tty: true
      command: ["/bin/sh","-c"]
      args:
        - |
          # Squash a warning.
          rm /etc/libibverbs.d/vmw_pvrdma.driver
          #################
          # Install vLLM
          #################
          /init-scripts/init-vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              #### MK
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              #### MK
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --kv-transfer-config \
                '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
              --enforce-eager
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              #### MK
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              #### MK
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --kv-transfer-config \
                '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
              --enforce-eager \
              --headless
          fi
      env:
        - name: DP_SIZE
          value: "{{ .DecodeDataParallelism }}"
        - name: TP_SIZE
          value: "{{ .DecodeTensorParallelism }}"
        - name: DP_SIZE_LOCAL
          value: "1"
        - name: VLLM_REPO_URL
          value: "https://github.com/vllm-project/vllm.git"
        - name: VLLM_BRANCH
          value: "main"
        - name: VLLM_ALL2ALL_BACKEND
#                    value: "naive"
          value: "pplx"
#                    value: "deepep_high_throughput"
#                    value: "deepep_low_latency"
#
          # Needed for GDRCOPY to be used.
          # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
#                  - name: NVIDIA_NVSWITCH
#                    value: "enabled"
#                  - name: NVIDIA_GDS
#                    value: "enabled"

        # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
#                  - name: NVIDIA_MOFED
#                    value: "enabled"
#
        - name: NCCL_DEBUG
          value: "INFO"
        - name: NVSHMEM_DEBUG
          value: "TRACE"
        - name: NVSHMEM_DEBUG_SUBSYS
          value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibrc"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
          value: "true"
        - name: NVSHMEM_HCA_LIST
          value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        - name: HF_TOKEN
          valueFrom:
            secretKeyRef:
              name: hf-secret
              key: HF_TOKEN
              optional: true
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add: [ "IPC_LOCK" ]
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 64Gi
          ephemeral-storage: 256Gi
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 64Gi
          ephemeral-storage: 256Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
      mountModelVolume: true
    
endpointPicker:
  image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
  debugLevel: 5
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2

  # enableService: true

  autoscaling:
    enabled: false
  replicas: 1

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}

# This section builds out the service account more information can be found here: https://kubernetes.io/docs/concepts/security/service-accounts/
eppServiceAccount:
  # Specifies whether a service account should be created
  create: true
  # Automatically mount a ServiceAccount's API credentials?
  automount: true
  # Annotations to add to the service account
  annotations: {}

