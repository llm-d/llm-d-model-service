# This values.yaml file creates the resources for deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct

# If true, use a LeaderWorkerSet instead of a Deployment to host the model
multinode: true
inferencePool: true
inferenceModel: true
httpRoute: true

routing: 
  # This is the model name for the OpenAI request
  modelName: deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct
  servicePort: 8080   # Sidecar listens on this port for requests. If there's no sidecar, the request goes here
  proxy:
    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
    targetPort: 8200
    debugLevel: 5
  parentRefs:
  - name: inference-gateway

modelArtifacts:
  # When specfying the URI with `hf` prefix, the <repo-id>/<model-id> string
  # is extracted and exposed as a template variable that can be used as {{ .HFModelName }}
  
  uri: "pvc://tms-hf-cache/model-cache"
  authSecretName: "hf-secret"
  size: 5Mi 

# describe decode pods
decode:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    tensor: 1
    data: 1
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  volumes:
    # Volume for the init script from ConfigMap
    - name: init-scripts-volume
      configMap:
        defaultMode: 0755
        name: vllm-init-scripts-config
    # Needed for NCCL to function
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    # - name: hf-cache
    #   persistentVolumeClaim:
    #     claimName: tms-hf-cache
    - name: vllm
      persistentVolumeClaim:
        claimName: tms-vllm

  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-pplx:0.1.0"
      imagePullPolicy: Always
      workingDir: /app
      stdin: true
      tty: true
      command: ["/bin/sh","-c"]
      args:
        - |
          #################
          # Install vLLM
          #################
          VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --enforce-eager
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              --port 8200 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --enforce-eager \
              --headless
          fi
      env:
        - name: CUDA_LAUNCH_BLOCKING
          value: "1"
        - name: VLLM_REPO_URL
          value: "https://github.com/vllm-project/vllm.git"
        - name: VLLM_BRANCH
          value: "main"
        #- name: VLLM_USE_DEEP_GEMM
        #  value: "1"
        - name: VLLM_ALL2ALL_BACKEND
        #  value: "naive"
          value: "pplx"
        #  value: "deepep_high_throughput"
        #  value: "deepep_low_latency"
        # Needed for GDRCOPY to be used.
        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        # Uncomment for debugging
        #- name: NVSHMEM_DEBUG_SUBSYS
        #  value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        #MK - name: HF_HUB_CACHE
        #MK   value: /huggingface-cache
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 64Gi
          ephemeral-storage: 64Gi
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 64Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
        #MK - name: hf-cache
        #MK   mountPath: /huggingface-cache
        - name: vllm
          mountPath: /code
      mountModelVolume: true

# describe prefill pods
prefill:
  autoscaling: 
    enabled: false
  replicas: 1

  parallelism:  
    tensor: 1
    data: 1
    dataLocal: 1 
  
  acceleratorTypes:
    labelKey: gpu.nvidia.com/model
    labelValues:
      - H200

  volumes:
    # Volume for the init script from ConfigMap
    - name: init-scripts-volume
      configMap:
        defaultMode: 0755
        name: vllm-init-scripts-config
    # Needed for NCCL to function
    - name: dshm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
    # - name: hf-cache
    #   persistentVolumeClaim:
    #     claimName: tms-hf-cache
    - name: vllm
      persistentVolumeClaim:
        claimName: tms-vllm


  containers:
    - name: vllm-worker
      image: "quay.io/tms/vllm-dev-pplx:0.1.0"
      imagePullPolicy: Always
      workingDir: /app
      stdin: true
      tty: true
      command: ["/bin/sh","-c"]
      args:
        - |
          #################
          # Install vLLM
          #################
          VLLM_USE_PRECOMPILED=1 /init-scripts/vllm.sh
          #################
          # RUN vLLM
          #################
          START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
          if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
            #################
            # Leader-only launch
            #################
            exec /app/venv/bin/vllm serve \
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              --port 8080 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --enforce-eager
          else
            #################
            # Worker-only launch
            #################
            exec /app/venv/bin/vllm serve \
              deepseek-ai/DeepSeek-Coder-V2-Lite-Instruct \
              --port 8080 \
              --disable-log-requests \
              --enable-expert-parallel \
              --tensor-parallel-size $TP_SIZE \
              --data-parallel-size $DP_SIZE \
              --data-parallel-size-local $DP_SIZE_LOCAL \
              --data-parallel-address $(LWS_LEADER_ADDRESS) \
              --data-parallel-rpc-port 5555 \
              --data-parallel-start-rank $START_RANK \
              --trust-remote-code \
              --headless
          fi
      env:
        - name: CUDA_LAUNCH_BLOCKING
          value: "1"
        - name: VLLM_REPO_URL
          value: "https://github.com/vllm-project/vllm.git"
        - name: VLLM_BRANCH
          value: "main"
        #- name: VLLM_USE_DEEP_GEMM
        #  value: "1"
        - name: VLLM_ALL2ALL_BACKEND
        #  value: "naive"
          value: "pplx"
        #  value: "deepep_high_throughput"
        #  value: "deepep_low_latency"
        # Needed for GDRCOPY to be used.
        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
        - name: NVIDIA_GDRCOPY
          value: "enabled"
        - name: NVSHMEM_DEBUG
          value: "INFO"
        # Uncomment for debugging
        #- name: NVSHMEM_DEBUG_SUBSYS
        #  value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
        - name: NVSHMEM_REMOTE_TRANSPORT
          value: "ibgda"
        - name: NVSHMEM_IB_ENABLE_IBGDA
          value: "true"
        - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
          value: "eth0"
        - name: GLOO_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_SOCKET_IFNAME
          value: "eth0"
        - name: NCCL_IB_HCA
          value: "ibp"
        - name: VLLM_LOGGING_LEVEL
          value: "DEBUG"
        #MK - name: HF_HUB_CACHE
        #MK   value: /huggingface-cache
        - name: GH_TOKEN_FROM_SECRET
          valueFrom:
            secretKeyRef:
              name: gh-token-secret
              key: GH_TOKEN
              optional: true
        - name: VLLM_NIXL_SIDE_CHANNEL_PORT
          value: "6555"
        - name: VLLM_NIXL_SIDE_CHANNEL_HOST
          valueFrom:
            fieldRef:
              fieldPath: status.podIP

      securityContext:
        capabilities:
          add:
          - "IPC_LOCK"
          - "SYS_RAWIO"
      resources:
        limits:
          nvidia.com/gpu: 1
          memory: 64Gi
          ephemeral-storage: 64Gi
          rdma/ib: 1
        requests:
          cpu: 8
          memory: 64Gi
          ephemeral-storage: 64Gi
          nvidia.com/gpu: 1
          rdma/ib: 1
      volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - name: init-scripts-volume
          mountPath: /init-scripts
        #MK - name: hf-cache
        #MK   mountPath: /huggingface-cache
        - name: vllm
          mountPath: /code
      mountModelVolume: true
    
endpointPicker:
  image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
  debugLevel: 5
  # This is for setting up a service more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/
  service:
    # This sets the service type more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#publishing-services-service-types
    type: ClusterIP
    # This sets the ports more information can be found here: https://kubernetes.io/docs/concepts/services-networking/service/#field-spec-ports
    port: 9002
    targetPort: 9002
    appProtocol: http2
  # The name of cluster role containing permissions to be granted to endpointPicker (via a role binding to the service account)
  permissions: pod-read

  autoscaling:
    enabled: false
  replicas: 1
