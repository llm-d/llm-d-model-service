# A universal baseconfig for models that can be downloaded from Hugging Face

# Requirements:
# Any consuming ModelService should define ports labeled:
#    - app_port - the external port number for the prefill and decode pods
#    - internal_port - the port number used by the sidecar to communicate with a vllm container
apiVersion: v1
kind: ConfigMap
metadata:
  name: lws-baseconfig
immutable: true
data:
  decodeLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-decode
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ .DecodeDataParallelism }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:    
                spec:
                  initContainers:
                  - name: routing-proxy
                    args:
                    - "--port={{ "app_port" | getPort }}"
                    - "--vllm-port={{ "internal_port" | getPort }}"
                    - --connector=nixlv2
                    - -v=6
                    image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
                    imagePullPolicy: Always
                    ports:
                    - containerPort: {{ "app_port" | getPort }}
                      protocol: TCP
                    resources: {}
                    restartPolicy: Always
                    securityContext:
                      allowPrivilegeEscalation: false
                      runAsNonRoot: true
                  containers:
                  - name: vllm-worker
                    image: "quay.io/tms/vllm-dev-base:0.0.15"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager \
                            --headless
                        fi
                    env:
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        value: "pplx"
    #                    value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: hf-secret
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    resources:
                      limits:
                        nvidia.com/gpu: "{{ .DecodeTensorParallelism }}"
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: "{{ .DecodeTensorParallelism }}"
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 1Gi

            workerTemplate:    
                spec:
                  containers:
                  - name: vllm-worker
                    image: "quay.io/tms/vllm-dev-base:0.0.15"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "internal_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager \
                            --headless
                        fi
                    env:
                      - name: DP_SIZE
                        value: "{{ .DecodeDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .DecodeTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        value: "pplx"
    #                    value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: hf-secret
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    resources:
                      limits:
                        nvidia.com/gpu: "{{ .DecodeTensorParallelism }}"
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: "{{ .DecodeTensorParallelism }}"
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 1Gi

  prefillLeaderWorkerSet: |
    apiVersion: leaderworkerset.x-k8s.io/v1
    kind: LeaderWorkerSet
    metadata:
        name: vllm-prefill
    spec:
        startupPolicy: LeaderCreated
        leaderWorkerTemplate:
            size: {{ .PrefillDataParallelism }}
            restartPolicy: RecreateGroupOnPodRestart

            leaderTemplate:

                spec:

                  containers:
                  - name: vllm-worker
                    image: "quay.io/tms/vllm-dev-base:0.0.15"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager \
                            --headless
                        fi
                    env:
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        value: "pplx"
    #                    value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: hf-secret
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    resources:
                      limits:
                        nvidia.com/gpu: "{{ .PrefillTensorParallelism }}"
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: "{{ .PrefillTensorParallelism }}"
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts
                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 1Gi

            workerTemplate:

                spec:

                  containers:
                  - name: vllm-worker
                    image: "quay.io/tms/vllm-dev-base:0.0.15"
                    imagePullPolicy: Always
                    workingDir: /app
                    stdin: true
                    tty: true
                    command: ["/bin/sh","-c"]
                    args:
                      - |
                        # Squash a warning.
                        rm /etc/libibverbs.d/vmw_pvrdma.driver
                        #################
                        # Install vLLM
                        #################
                        /init-scripts/init-vllm.sh
                        #################
                        # RUN vLLM
                        #################
                        START_RANK=$(( ${LWS_WORKER_INDEX:-0} * DP_SIZE_LOCAL ))
                        if [ "${LWS_WORKER_INDEX:-0}" -eq 0 ]; then
                          #################
                          # Leader-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager
                        else
                          #################
                          # Worker-only launch
                          #################
                          exec /app/venv/bin/vllm serve \
                            {{ .HFModelName }} \
                            --port {{ "app_port" | getPort }} \
                            --disable-log-requests \
                            --enable-expert-parallel \
                            --tensor-parallel-size $TP_SIZE \
                            --data-parallel-size $DP_SIZE \
                            --data-parallel-size-local $DP_SIZE_LOCAL \
                            --data-parallel-address $(LWS_LEADER_ADDRESS) \
                            --data-parallel-rpc-port 5555 \
                            --data-parallel-start-rank $START_RANK \
                            --trust-remote-code \
                            --kv-transfer-config \
                              '{"kv_connector":"NixlConnector","kv_role":"kv_both"}' \
                            --enforce-eager \
                            --headless
                        fi
                    env:
                      - name: DP_SIZE
                        value: "{{ .PrefillDataParallelism }}"
                      - name: TP_SIZE
                        value: "{{ .PrefillTensorParallelism }}"
                      - name: DP_SIZE_LOCAL
                        value: "1"
                      - name: VLLM_REPO_URL
                        value: "https://github.com/vllm-project/vllm.git"
                      - name: VLLM_BRANCH
                        value: "main"
                      - name: VLLM_ALL2ALL_BACKEND
    #                    value: "naive"
                        value: "pplx"
    #                    value: "deepep_high_throughput"
    #                    value: "deepep_low_latency"
    #
                        # Needed for GDRCOPY to be used.
                        # See: https://github.com/NVIDIA/nvidia-container-toolkit/releases/tag/v1.15.0
                      - name: NVIDIA_GDRCOPY
                        value: "enabled"
    #                  - name: NVIDIA_NVSWITCH
    #                    value: "enabled"
    #                  - name: NVIDIA_GDS
    #                    value: "enabled"

                      # NVIDIA_MOFED is likely needed for using IBGDA but causes crashes
    #                  - name: NVIDIA_MOFED
    #                    value: "enabled"
    #
                      - name: NCCL_DEBUG
                        value: "INFO"
                      - name: NVSHMEM_DEBUG
                        value: "TRACE"
                      - name: NVSHMEM_DEBUG_SUBSYS
                        value: "TRANSPORT,INIT,MEM,COLL,BOOTSTRAP"
                      - name: NVSHMEM_REMOTE_TRANSPORT
                        value: "ibrc"
                      - name: NVSHMEM_IB_ENABLE_IBGDA
                        value: "true"
                      - name: NVSHMEM_ENABLE_NIC_PE_MAPPING
                        value: "true"
                      - name: NVSHMEM_HCA_LIST
                        value: "ibp0:1,ibp1:1,ibp2:1,ibp3:1,ibp4:1,ibp5:1,ibp6:1,ibp7:1"
                      - name: NVSHMEM_BOOTSTRAP_UID_SOCK_IFNAME
                        value: "eth0"
                      - name: GLOO_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_SOCKET_IFNAME
                        value: "eth0"
                      - name: NCCL_IB_HCA
                        value: "ibp"
                      - name: VLLM_LOGGING_LEVEL
                        value: "DEBUG"
                      - name: HF_TOKEN
                        valueFrom:
                          secretKeyRef:
                            name: hf-secret
                            key: HF_TOKEN
                            optional: true
                      - name: GH_TOKEN_FROM_SECRET
                        valueFrom:
                          secretKeyRef:
                            name: gh-token-secret
                            key: GH_TOKEN
                            optional: true
                      - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                        value: "6555"
                      - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                        valueFrom:
                          fieldRef:
                            fieldPath: status.podIP

                    securityContext:
                      capabilities:
                        add: [ "IPC_LOCK" ]
                    resources:
                      limits:
                        nvidia.com/gpu: "{{ .PrefillTensorParallelism }}"
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        rdma/ib: 1
                      requests:
                        cpu: 8
                        memory: 64Gi
                        ephemeral-storage: 256Gi
                        nvidia.com/gpu: "{{ .PrefillTensorParallelism }}"
                        rdma/ib: 1
                    volumeMounts:
                      - mountPath: /dev/shm
                        name: dshm
                      - name: init-scripts-volume
                        mountPath: /init-scripts

                  volumes:
                    # Volume for the init script from ConfigMap
                    - name: init-scripts-volume
                      configMap:
                        name: vllm-init-scripts-config
                        defaultMode: 0755 # Set execute permissions for the script
                    # Needed for NCCL to function
                    - name: dshm
                      emptyDir:
                        medium: Memory
                        sizeLimit: 1Gi

  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        # Needs to match the port of the eppDeployment
        - port: 9002    
          protocol: TCP
          targetPort: 9002 
          appProtocol: http2
      type: ClusterIP
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - llmd-kalantar
                - -v
                - "5"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: PD_ENABLED
                  value: "true"
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: "10"
              image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: {{ "app_port" | getPort }}
      selector:
        leaderworkerset.sigs.k8s.io/worker-index: "0"

  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel

  httpRoute: |
    apiVersion: gateway.networking.k8s.io/v1
    kind: HTTPRoute
    spec:
      parentRefs:
      - group: gateway.networking.k8s.io
        kind: Gateway
        name: inference-gateway
      rules:
      - backendRefs:
        - group: inference.networking.x-k8s.io
          kind: InferencePool
          name: {{ .InferencePoolName }}
          port: {{ "app_port" | getPort }}