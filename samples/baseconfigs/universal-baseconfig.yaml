# Based on: https://github.com/llm-d/llm-d-routing-sidecar/tree/dev/test/config/nixl
#
# Requirements:
# Any consuming ModelService should define ports labeled:
#    - app_port - the external port number for the prefill and decode pods
#    - internal_port - the port number used by the sidecar to communicate with a vllm container
apiVersion: v1
kind: ConfigMap
metadata:
  name: universal-base-config
immutable: true
data:
  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          initContainers:
            - name: routing-proxy
              image: quay.io/llm-d/llm-d-routing-sidecar:0.0.5
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
              args:
                # Note: this port has to match the prefill port
                - "--port={{ "app_port" | getPort }}"
                - "--vllm-port={{ "internal_port" | getPort }}"
                - "--connector=nixl"
              ports:
                - containerPort: {{ "app_port" | getPort }}
                  protocol: TCP
              restartPolicy: Always
          containers:
            - name: vllm
              image: quay.io/llm-d/llm-d:0.0.6
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                - "--port"
                - "{{ "internal_port" | getPort }}"
                - "--enforce-eager"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: NIXL_ROLE
                  value: RECVER
                - name: HF_HUB_CACHE
                  value: /vllm-workspace/models
              ports:
                - containerPort: 55555
                  protocol: TCP
              volumeMounts:
                - name: model-cache
                  mountPath: /vllm-workspace/models
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: model-cache
              emptyDir:
                sizeLimit: 20Gi

  prefillDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: vllm
              image: quay.io/llm-d/llm-d:0.0.6
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                # Note: this port has to match the proxy --port arg
                - "--port"
                - "{{ "app_port" | getPort }}"
                - "--enforce-eager"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector","kv_role":"kv_both"}'
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: VLLM_LOGGING_LEVEL
                  value: DEBUG
                - name: HF_HUB_CACHE
                  value: /vllm-workspace/models
              ports:
                - containerPort: {{ "app_port" | getPort }}
                  protocol: TCP
                - containerPort: 5557
                  protocol: TCP
              volumeMounts:
                - name: model-cache
                  mountPath: /vllm-workspace/models
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: model-cache
              emptyDir:
                sizeLimit: 20Gi
  
  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        - port: 9002    # Needs to match the port of the eppDeployment
          protocol: TCP
      type: NodePort
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - {{ .ModelServiceNamespace }}
                - -v
                - "4"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: PD_ENABLED
                  value: "true"
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: "10"
              image: quay.io/llm-d/llm-d-gateway-api-inference-extension-dev:0.0.5
              imagePullPolicy: Always
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: {{ .EPPServiceName }}
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: {{ .EPPServiceName }}
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: {{ "app_port" | getPort }}
  
  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel
