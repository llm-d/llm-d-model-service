# Source: https://github.com/neuralmagic/llm-d-routing-sidecar/tree/dev/test/config/nixl
apiVersion: v1
kind: ConfigMap
metadata:
  name: universal-base-config
immutable: true
data:
  configMaps: |
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: {{ .SanitizedModelName }}-config-decoder
      data:
        lmcache-decoder-config.yaml: |
          local_cpu: False
          max_local_cpu_size: 0
          #local_disk:
          max_local_disk_size: 0
          remote_serde: NULL

          enable_nixl: True
          nixl_role: "receiver"
          nixl_peer_host: "0.0.0.0"
          nixl_peer_port: 55555
          nixl_buffer_size: 524288
          nixl_buffer_device: "cuda"
          nixl_enable_gc: True
    - apiVersion: v1
      kind: ConfigMap
      metadata:
        name: {{ .SanitizedModelName }}-config-prefiller
      data:
        lmcache-prefiller-config.yaml: |
          local_cpu: False
          max_local_cpu_size: 0
          #local_disk:
          max_local_disk_size: 0
          remote_serde: NULL

          enable_nixl: True
          nixl_role: "sender"
          nixl_peer_host: "{{ .SanitizedModelName }}-decoder"
          nixl_peer_port: 55555
          nixl_buffer_size: 524288
          nixl_buffer_device: "cuda"
          nixl_enable_gc: True

  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          imagePullSecrets:
            - name: quay-secret-llm-d
          initContainers:
            - name: routing-proxy
              image: quay.io/llm-d/llm-d-routing-sidecar@sha256:2682e3ad86d2e741030eb693544e1977f5ce072fe6e3db78ffa18ddf84b6dd64
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
              args:
                # Note: this port has to match the prefill port
                - "--port=8000"
                - "--vllm-port=8200"
              ports:
                - containerPort: 8000
                  protocol: TCP
              restartPolicy: Always
          containers:
            - name: vllm
              image: quay.io/llm-d/llm-d-dev@sha256:d6d212de0d1dc0f6da9877eab21800f62d7dd32d825bae9bf1692c4f6e017109
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                - "--port"
                - "8200"
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: HF_HUB_CACHE
                  value: /vllm-workspace/models
              ports:
                - containerPort: 55555
                  protocol: TCP
              volumeMounts:
                - name: config-decoder
                  mountPath: /vllm-workspace
                - name: model-cache
                  mountPath: /vllm-workspace/models
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: config-decoder
              configMap:
                name: {{ .SanitizedModelName }}-config-decoder
            - name: model-cache
              emptyDir:
                sizeLimit: 1Gi

  prefillDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          imagePullSecrets:
            - name: quay-secret-llm-d
          containers:
            - name: vllm
              image: quay.io/llm-d/llm-d-dev@sha256:d6d212de0d1dc0f6da9877eab21800f62d7dd32d825bae9bf1692c4f6e017109
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                # Note: this port has to match the proxy --port arg
                - "--port"
                - "8000"
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: HF_HUB_CACHE
                  value: /vllm-workspace/models
              ports:
                - containerPort: 8000
                  protocol: TCP
                - containerPort: 5557
                  protocol: TCP
              volumeMounts:
                - name: config-prefiller
                  mountPath: /vllm-workspace
                - name: model-cache
                  mountPath: /vllm-workspace/models
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: config-prefiller
              configMap:
                name: {{ .SanitizedModelName }}-config-prefiller
            - name: model-cache
              emptyDir:
                sizeLimit: 1Gi

  decodeService: |
    apiVersion: v1
    kind: Service
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 55555
        protocol: TCP
      - name: vllm
        port: 8000
        protocol: TCP
  
  prefillService: |
    apiVersion: v1
    kind: Service
    spec:
      clusterIP: None
      ports:
      - name: nixl
        port: 55555
        protocol: TCP
      - name: vllm
        port: 8000
        protocol: TCP
  
  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        - port: 9002    # Needs to match the port of the eppDeployment
          protocol: TCP
      type: NodePort
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                # TODO: replace with template vars
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - {{ .ModelServiceNamespace }}
                - -v
                - "4"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: PD_ENABLED
                  value: "true"
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: "10"
              image: quay.io/llm-d/llm-d-gateway-api-inference-extension-dev:0.0.5
              imagePullPolicy: Always
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: {{ .EPPServiceName }}
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: {{ .EPPServiceName }}
                initialDelaySeconds: 5
                periodSeconds: 10
                successThreshold: 1
                timeoutSeconds: 1
          imagePullSecrets:
            - name: quay-secret-llm-d
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: 8000
  
  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel
