# A universal baseconfig for models that can be downloaded from Hugging Face

# Requirements:
# Any consuming ModelService should define ports labeled:
#    - app_port - the external port number for the prefill and decode pods
#    - internal_port - the port number used by the sidecar to communicate with a vllm container
apiVersion: v1
kind: ConfigMap
metadata:
  name: universal-base-config-hf
immutable: true
data:
  decodeDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          initContainers:
            - name: routing-proxy
              image: ghcr.io/llm-d/llm-d-routing-sidecar:0.0.6
              imagePullPolicy: Always
              securityContext:
                allowPrivilegeEscalation: false
                runAsNonRoot: true
              args:
                # Note: this port has to match the prefill port
                - "--port={{ "app_port" | getPort }}"
                - "--vllm-port={{ "internal_port" | getPort }}"
                - "--connector=nixlv2"
                - "-v=6"
              ports:
                - containerPort: {{ "app_port" | getPort }}
                  protocol: TCP
              restartPolicy: Always
              imagePullPolicy: Always
          containers:
            - name: vllm
              image: ghcr.io/llm-d/llm-d:0.0.8
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                - "--port"
                - "{{ "internal_port" | getPort }}"
                - "--enforce-eager"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: HF_HOME
                  value: /model-cache
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_LOGGING_LEVEL
                  value: DEBUG
              ports:
                - containerPort: 5557
                  protocol: TCP
              volumeMounts:
                - name: model-storage
                  mountPath: /model-cache
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: model-storage
              emptyDir:
                sizeLimit: 20Gi

  prefillDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: vllm
              image: ghcr.io/llm-d/llm-d:0.0.8
              command:
                - vllm
                - serve
              securityContext:
                allowPrivilegeEscalation: false
              args:
                # Note: this port has to match the proxy --port arg
                - "--port"
                - "{{ "app_port" | getPort }}"
                - "--enforce-eager"
                - "--kv-transfer-config"
                - '{"kv_connector":"NixlConnector", "kv_role":"kv_both"}'
              env:
                - name: CUDA_VISIBLE_DEVICES
                  value: "0"
                - name: UCX_TLS
                  value: "cuda_ipc,cuda_copy,tcp"
                - name: VLLM_NIXL_SIDE_CHANNEL_PORT
                  value: "5557"
                - name: VLLM_NIXL_SIDE_CHANNEL_HOST
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: VLLM_LOGGING_LEVEL
                  value: DEBUG
                - name: HF_HOME
                  value: /model-cache
              ports:
                - containerPort: {{ "app_port" | getPort }}
                  protocol: TCP
                - containerPort: 5557
                  protocol: TCP
              volumeMounts:
                - name: model-storage
                  mountPath: /model-cache
              resources:
                limits:
                  nvidia.com/gpu: 1
                requests:
                  cpu: "16"
                  memory: 16Gi
                  nvidia.com/gpu: 1
          volumes:
            - name: model-storage
              emptyDir:
                sizeLimit: 20Gi
  
  eppService: |
    apiVersion: v1
    kind: Service
    spec:
      ports:
        # Needs to match the port of the eppDeployment
        - port: 9002    
          protocol: TCP
          targetPort: 9002 
          appProtocol: http2
      type: ClusterIP
  
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    spec:
      template:
        spec:
          containers:
            - name: "epp"
              args:
                - -poolName
                - {{ .InferencePoolName }}
                - -poolNamespace
                - {{ .ModelServiceNamespace }}
                - -v
                - "4"
                - --zap-encoder
                - json
                - -grpcPort
                - "9002"
                - -grpcHealthPort
                - "9003"
              env:
                - name: PD_ENABLED
                  value: "true"
                - name: PD_PROMPT_LEN_THRESHOLD
                  value: "10"
              image: ghcr.io/llm-d/llm-d-inference-scheduler:0.0.3
              livenessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
              ports:
                - containerPort: 9002
                  protocol: TCP
                - containerPort: 9003
                  protocol: TCP
                - containerPort: 9090
                  name: metrics
                  protocol: TCP
              readinessProbe:
                failureThreshold: 3
                grpc:
                  port: 9003
                  service: envoy.service.ext_proc.v3.ExternalProcessor
                initialDelaySeconds: 5
                periodSeconds: 10
  
  inferencePool: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferencePool
    spec:
      targetPortNumber: {{ "app_port" | getPort }}
  
  inferenceModel: |
    apiVersion: inference.networking.x-k8s.io/v1alpha2
    kind: InferenceModel

  httpRoute: |
    apiVersion: gateway.networking.k8s.io/v1
    kind: HTTPRoute
    spec:
      parentRefs:
      - group: gateway.networking.k8s.io
        kind: Gateway
        name: inference-gateway
      rules:
      - backendRefs:
        - group: inference.networking.x-k8s.io
          kind: InferencePool
          name: {{ .InferencePoolName }}
          port: {{ "app_port" | getPort }}