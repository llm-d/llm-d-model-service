apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: busybox
spec:
  decoupleScaling: false

  baseConfigMapRef:
    name: basic-basic-conf

  routing: 
    modelName: llama-2075
    ports:
    - name: inport
      port: 80
    - name: outport
      port: 9376
    gatewayRefs:
    - name: inference-gateway-name
      port: 1112

  modelArtifacts:
    # uri: pvc://llama-of-the-future/path/to/llama-2075
    uri: oci+native://redhat/granite-7b-lab-gguf:1.0::/

  # describe decode pods
  decode:
    replicas: 1
    initContainers:
    - name: "proxy"
      image: "busybox"
      args: 
      - "{{ .ModelPath }}"
      mountModelVolume: true 
      
    containers:
    - name: "llm"
      image: busybox
      args:
      - "{{ .ModelName }}"

  endpointPicker:
    containers:
    - name: "epp"
      env:
      - name: HF_TOKEN
        value: hello
      - name: USE_STREAMING
        value: "false"