apiVersion: v1
kind: ConfigMap
metadata:
  name: basic-basic-conf
data:
  decodeDeployment: |
    spec:
      replicas: 2
      template:
        spec:
          containers:
          - name: llm
            command:
            - sleep

  # Note that this label is preserved and our labels are added
  decodeService: |
    spec:
      selector:
        app.kubernetes.io/name: decodeServiceLabelInBaseConfig
      ports:
        - protocol: TCP
          port: {{ "inport" | getPort }}
          targetPort: {{ "outport" | getPort }}
  
  # This service should not be created bc prefill doesn't exist in basemsvc.yaml
  prefillService: |
    spec:
      selector:
        app.kubernetes.io/name: prefillServiceLabelInBaseConfig
      ports:
        - protocol: TCP
          port: {{ "inport" | getPort }}
          targetPort: {{ "outport" | getPort }}

  inferenceModel: |
    spec:
      criticality: Standard    
  
  inferencePool: |
    spec:
      targetPortNumber: {{ "outport" | getPort }}
  eppDeployment: |
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: epp
      namespace: default
    spec:
      replicas: 1
      template:
        spec:
          # Conservatively, this timeout should mirror the longest grace period of the pods within the pool
          terminationGracePeriodSeconds: 130
          containers:
          - name: epp
            image: us-central1-docker.pkg.dev/k8s-staging-images/gateway-api-inference-extension/epp:main
            imagePullPolicy: Always
            args:
            - -poolName
            - my-pool-name
            - -poolNamespace
            - my-pool-namespace
            - -v
            - "4"
            - --zap-encoder
            - "json"
            - -grpcPort
            - "9002"
            - -grpcHealthPort
            - "9003"
            env:
            - name: USE_STREAMING
              value: "true"
            ports:
            - containerPort: 9002
            - containerPort: 9003
            - name: metrics
              containerPort: 9090
            livenessProbe:
              grpc:
                port: 9003
                service: inference-extension
              initialDelaySeconds: 5
              periodSeconds: 10
            readinessProbe:
              grpc:
                port: 9003
                service: inference-extension
              initialDelaySeconds: 5
              periodSeconds: 10 
  eppService: |
    apiVersion: v1
    kind: Service
    metadata:
      name: llm-llama3-8b-instruct-epp
      namespace: default
    spec:
      selector:
        app: llm-llama3-8b-instruct-epp
      ports:
      - protocol: TCP
        port: 9002
        targetPort: 9002
        appProtocol: http2
    type: ClusterIP   
  
  httpRoute: |
    apiVersion: gateway.networking.k8s.io/v1
    kind: HTTPRoute
    spec:
      parentRefs:
      - name: inference-gateway-name
        port: 1234
      rules:
      - matches:
        - path:
            type: PathPrefix
            value: /
      - backendRefs:
        - group: inference.networking.x-k8s.io
          kind: InferencePool
          name: {{ .InferencePoolName }}
          port: {{ "outport" | getPort }}
