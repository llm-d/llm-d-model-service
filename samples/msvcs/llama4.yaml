apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: meta-llama-4-scout-17b-16e
spec:
  decoupleScaling: false

  baseConfigMapRef:
    name: universal-base-config-pvc

  routing: 
    modelName: meta-llama/Llama-4-Scout-17B-16E

  modelArtifacts:
    uri: pvc://llama-pvc/path/to/llama4

  # describe decode pods
  decode:
    replicas: 1
    parallelism:  
      tensor: 8
    containers:
    - name: "vllm"
      args:
      # Comes from model-storage volume, which is a PVC created by MSVC controller
      # The mountPath is /cache
      # {{ .ModelPath }} == /path/to/llama4
      - '/cache/{{ .ModelPath }}'
      
      # Other args come from https://blog.vllm.ai/2025/04/05/llama4.html
      # This is for reference only
      # Modify the args as you wish
      - "--tensor-parallel-size"
      - "8"
      - "--max-model-len" 
      - "1000000"
      - "--override-generation-config='{\"attn_temperature_tuning\": true}'"
      
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues:
        # According to the blog, Scout requires H100s
        - NVIDIA-H100
  
  # describe the prefill pods 
  prefill:
    replicas: 1
    parallelism:  
      tensor: 8
    containers:
    - name: "vllm"
      args:
      - '/cache/{{ .ModelPath }}'
      - "--tensor-parallel-size"
      - "8"
      - "--max-model-len" 
      - "1000000"
      - "--override-generation-config='{\"attn_temperature_tuning\": true}'"
      
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues:
        - NVIDIA-H100
