apiVersion: llm-d.ai/v1alpha1
kind: ModelService
metadata:
  name: facebook-opt-125m-nixl
spec:
  decoupleScaling: false

  baseConfigMapRef:
    name: universal-base-config

  routing: 
    modelName: facebook/opt-125m

  modelArtifacts:
    uri: hf://facebook/opt-125m

  # describe decode pods
  decode:
    replicas: 1
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues:
        - NVIDIA-A100-SXM4-80GB
    containers:
    - name: "vllm"
      # The baseconfig image includes LMCache and multiconnector support
      args:
        - "{{ .HFModelName }}"
  
  # describe the prefill pods 
  prefill:
    replicas: 1
    acceleratorTypes:
      labelKey: nvidia.com/gpu.product
      labelValues:
        - NVIDIA-A100-SXM4-80GB
    containers:
      - name: "vllm"
        args:
          - "{{ .HFModelName }}"
      
